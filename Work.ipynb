{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429d497a-ee79-48be-afa6-27aea8ff58f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T03:58:53.907585Z",
     "iopub.status.busy": "2023-04-24T03:58:53.906922Z",
     "iopub.status.idle": "2023-04-24T03:58:56.986750Z",
     "shell.execute_reply": "2023-04-24T03:58:56.985857Z",
     "shell.execute_reply.started": "2023-04-24T03:58:53.907555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 1)) (1.23.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 2)) (1.4.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: dtaidistance in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 4)) (2.3.10)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 5)) (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 6)) (0.13.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 7)) (0.12.0+cu116)\n",
      "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 8)) (2.1.1)\n",
      "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 9)) (0.6.17)\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 10)) (2.3.0)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 11)) (2.6)\n",
      "Requirement already satisfied: dgl in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 13)) (2.8.4)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 14)) (2.9.1)\n",
      "Requirement already satisfied: pymetis in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 15)) (2023.1)\n",
      "Requirement already satisfied: ogb in /usr/local/lib/python3.9/dist-packages (from -r FedStar/requirements.txt (line 16)) (1.3.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r FedStar/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r FedStar/requirements.txt (line 2)) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r FedStar/requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r FedStar/requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r FedStar/requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r FedStar/requirements.txt (line 5)) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->-r FedStar/requirements.txt (line 6)) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->-r FedStar/requirements.txt (line 6)) (2.28.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric->-r FedStar/requirements.txt (line 10)) (5.9.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric->-r FedStar/requirements.txt (line 10)) (3.0.9)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric->-r FedStar/requirements.txt (line 10)) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric->-r FedStar/requirements.txt (line 10)) (3.1.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX->-r FedStar/requirements.txt (line 11)) (3.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX->-r FedStar/requirements.txt (line 11)) (21.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (2.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (3.7.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (63.1.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (2.9.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (0.26.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.12)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r FedStar/requirements.txt (line 14)) (1.47.0)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ogb->-r FedStar/requirements.txt (line 16)) (0.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb->-r FedStar/requirements.txt (line 16)) (1.26.10)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->-r FedStar/requirements.txt (line 14)) (0.35.1)\n",
      "Requirement already satisfied: littleutils in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb->-r FedStar/requirements.txt (line 16)) (0.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->-r FedStar/requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r FedStar/requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->-r FedStar/requirements.txt (line 6)) (2019.11.28)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (2.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (3.3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric->-r FedStar/requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->-r FedStar/requirements.txt (line 14)) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r FedStar/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b18fbe5-9cbc-434a-866e-813962050d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T05:56:57.836890Z",
     "iopub.status.busy": "2023-04-24T05:56:57.836109Z",
     "iopub.status.idle": "2023-04-24T05:57:00.700929Z",
     "shell.execute_reply": "2023-04-24T05:57:00.699950Z",
     "shell.execute_reply.started": "2023-04-24T05:56:57.836798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#generic\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "from random import choices\n",
    "import pickle\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv, global_add_pool, SAGEConv\n",
    "from torch_geometric.transforms import OneHotDegree\n",
    "from torch_geometric.utils import to_networkx, degree, to_dense_adj, to_scipy_sparse_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse as sp\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "from torch_geometric.utils import to_networkx, subgraph\n",
    "import torch_geometric.utils as utils\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "#utility\n",
    "import networkx as nx\n",
    "from dtaidistance import dtw\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pymetis\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "num_clients = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "alg = 'fedstar'\n",
    "num_rounds = 20\n",
    "local_epoch = 10\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "nlayer = 3 # number of GINConv layers\n",
    "hidden = 64\n",
    "dropout = 0.5\n",
    "batch_size = 128  # not used\n",
    "seed = 69\n",
    "datapath = '.Data'\n",
    "outbase = 'outputs'\n",
    "data_group = 'arxiv'\n",
    "n_rw = 16\n",
    "n_dg = 16\n",
    "n_ones = 16\n",
    "type_init = 'rw_dg' #options are rw, dg and rw_dg\n",
    "print(device)\n",
    "seed_dataSplit = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de33bd3d-c119-42aa-b998-6f7c61d29a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T05:57:00.702987Z",
     "iopub.status.busy": "2023-04-24T05:57:00.702450Z",
     "iopub.status.idle": "2023-04-24T05:57:00.718830Z",
     "shell.execute_reply": "2023-04-24T05:57:00.717851Z",
     "shell.execute_reply.started": "2023-04-24T05:57:00.702954Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_numGraphLabels(g):\n",
    "    s = set(g.y.flatten().tolist())\n",
    "    return len(s)\n",
    "\n",
    "def init_structure_encoding(  g, type_init = 'rw_dg'):\n",
    "\n",
    "    if type_init == 'rw':\n",
    "        # Geometric diffusion features with Random Walk\n",
    "        A = to_scipy_sparse_matrix(g.edge_index, num_nodes=g.num_nodes)\n",
    "        D = (degree(g.edge_index[0], num_nodes=g.num_nodes) ** -1.0).numpy()\n",
    "\n",
    "        Dinv=sp.diags(D)\n",
    "        RW=A*Dinv\n",
    "        M=RW\n",
    "\n",
    "        SE_rw=[torch.from_numpy(M.diagonal()).float()]\n",
    "        M_power=M\n",
    "        for _ in range(n_rw-1):\n",
    "            M_power=M_power*M\n",
    "            SE_rw.append(torch.from_numpy(M_power.diagonal()).float())\n",
    "        SE_rw=torch.stack(SE_rw,dim=-1)\n",
    "\n",
    "        g['stc_enc'] = SE_rw\n",
    "\n",
    "    elif type_init == 'dg':\n",
    "        # PE_degree\n",
    "        g_dg = (degree(g.edge_index[0], num_nodes=g.num_nodes)).numpy().clip(1, n_dg)\n",
    "        SE_dg = torch.zeros([g.num_QCnodes, n_dg])\n",
    "        for i in range(len(g_dg)):\n",
    "            SE_dg[i,int(g_dg[i]-1)] = 1\n",
    "\n",
    "        g['stc_enc'] = SE_dg\n",
    "\n",
    "    elif type_init == 'rw_dg':\n",
    "        # SE_rw\n",
    "        A = to_scipy_sparse_matrix(g.edge_index, num_nodes=g.num_nodes)\n",
    "        D = (degree(g.edge_index[0], num_nodes=g.num_nodes) ** -1.0).numpy()\n",
    "\n",
    "        Dinv=sp.diags(D)\n",
    "        RW=A*Dinv\n",
    "        M=RW\n",
    "\n",
    "        SE=[torch.from_numpy(M.diagonal()).float()]\n",
    "        M_power=M\n",
    "        for _ in range(n_rw-1):\n",
    "            M_power=M_power*M\n",
    "            SE.append(torch.from_numpy(M_power.diagonal()).float())\n",
    "        SE_rw=torch.stack(SE,dim=-1)\n",
    "\n",
    "        # PE_degree\n",
    "        g_dg = (degree(g.edge_index[0], num_nodes=g.num_nodes)).numpy().clip(1, n_dg)\n",
    "        SE_dg = torch.zeros([g.num_nodes, n_dg])\n",
    "        for i in range(len(g_dg)):\n",
    "            SE_dg[i,int(g_dg[i]-1)] = 1\n",
    "\n",
    "        g['stc_enc'] = torch.cat([SE_rw, SE_dg], dim=1)\n",
    "\n",
    "    return g\n",
    "\n",
    "def get_stats(df, ds, graph_train, graph_val=None, graph_test=None):\n",
    "    from collections import Counter\n",
    "    labels_train = graph_train.y.flatten().tolist()\n",
    "    df.loc[ds, '#Nodes_train'] = graph_train.num_nodes\n",
    "    df.loc[ds, '#Edges_train'] = graph_train.num_edges\n",
    "    df.loc[ds, 'Avg_degree_train'] = graph_train.num_edges/graph_train.num_nodes\n",
    "    df.loc[ds, '#Labels_train'] = len(set(labels_train))\n",
    "    df.loc[ds, 'Class_dist_train'] = str(dict(Counter(labels_train)))\n",
    "    \n",
    "    if graph_test:\n",
    "        labels_test = graph_test.y.flatten().tolist()\n",
    "        df.loc[ds, '#Nodes_test'] = graph_test.num_nodes\n",
    "        df.loc[ds, '#Edges_test'] = graph_test.num_edges\n",
    "        df.loc[ds, 'Avg_degree_test'] = graph_test.num_edges/graph_test.num_nodes\n",
    "        df.loc[ds, '#Labels_test'] = len(set(labels_test))\n",
    "        df.loc[ds, 'Class_dist_test'] = str(dict(Counter(labels_test)))\n",
    "        \n",
    "    if graph_val:\n",
    "        labels_val = graph_val.y.flatten().tolist()\n",
    "        df.loc[ds, '#Nodes_val'] = graph_val.num_nodes\n",
    "        df.loc[ds, '#Edges_val'] = graph_val.num_edges\n",
    "        df.loc[ds, 'Avg_degree_val'] = graph_val.num_edges/graph_val.num_nodes\n",
    "        df.loc[ds, '#Labels_val'] = len(set(labels_val))\n",
    "        df.loc[ds, 'Class_dist_val'] = str(dict(Counter(labels_val)))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390e2f5f-8429-456e-a151-a692c2a31890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T05:57:00.720254Z",
     "iopub.status.busy": "2023-04-24T05:57:00.719965Z",
     "iopub.status.idle": "2023-04-24T05:57:01.238498Z",
     "shell.execute_reply": "2023-04-24T05:57:01.237524Z",
     "shell.execute_reply.started": "2023-04-24T05:57:00.720227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1], stc_enc=[169343, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('graph_struc_pickle', 'rb')\n",
    "graph = pickle.load(file)\n",
    "file.close()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca48f65-fe54-46b7-b439-76b0a6731c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T05:57:01.479211Z",
     "iopub.status.busy": "2023-04-24T05:57:01.478413Z",
     "iopub.status.idle": "2023-04-24T05:57:03.455455Z",
     "shell.execute_reply": "2023-04-24T05:57:03.454830Z",
     "shell.execute_reply.started": "2023-04-24T05:57:01.479181Z"
    }
   },
   "outputs": [],
   "source": [
    "class GIN_dc(torch.nn.Module):\n",
    "    def __init__(self, nfeat, n_se, nhid, nclass, nlayer, dropout):\n",
    "        super(GIN_dc, self).__init__()\n",
    "        self.num_layers = nlayer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.pre = torch.nn.Sequential(torch.nn.Linear(nfeat, nhid))\n",
    "\n",
    "        self.embedding_s = torch.nn.Linear(n_se, nhid)\n",
    "\n",
    "        self.graph_convs = torch.nn.ModuleList()\n",
    "        self.nn1 = torch.nn.Sequential(torch.nn.Linear(nhid + nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "        self.graph_convs.append(GINConv(self.nn1))\n",
    "        self.graph_convs_s_gcn = torch.nn.ModuleList()\n",
    "        self.graph_convs_s_gcn.append(GCNConv(nhid, nhid))\n",
    "\n",
    "        for l in range(nlayer - 1):\n",
    "            self.nnk = torch.nn.Sequential(torch.nn.Linear(nhid + nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "            self.graph_convs.append(GINConv(self.nnk))\n",
    "            self.graph_convs_s_gcn.append(GCNConv(nhid, nhid))\n",
    "\n",
    "        self.Whp = torch.nn.Linear(nhid + nhid, nhid)\n",
    "        self.post = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU())\n",
    "        self.readout = torch.nn.Sequential(torch.nn.Linear(nhid, nclass))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, s = data.x, data.edge_index, data.stc_enc\n",
    "        x = self.pre(x)\n",
    "        s = self.embedding_s(s)\n",
    "        for i in range(len(self.graph_convs)):\n",
    "            x = torch.cat((x, s), -1)\n",
    "            x = self.graph_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            s = self.graph_convs_s_gcn[i](s, edge_index)\n",
    "            s = torch.tanh(s)\n",
    "        x = self.Whp(torch.cat((x, s), -1))\n",
    "        x = self.post(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.readout(x)\n",
    "        # print(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # print(x)\n",
    "        return x.float()\n",
    "    def loss(self, pred, label):\n",
    "        # print(pred, label)\n",
    "        return F.cross_entropy(pred, label)\n",
    "        # return F.nll_loss(pred, label)\n",
    "data = copy.deepcopy(graph)\n",
    "num_classes = get_numGraphLabels(data)\n",
    "n_se = n_rw+n_dg\n",
    "data.y = one_hot(data.y).squeeze(dim=1).float()\n",
    "model = GIN_dc(nfeat=data.num_node_features, n_se=n_se, nhid=64, nclass=num_classes, nlayer=3, dropout=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f43afa-635f-4e76-840b-a29a445d0ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T06:08:33.528769Z",
     "iopub.status.busy": "2023-04-24T06:08:33.527903Z",
     "iopub.status.idle": "2023-04-24T06:08:33.539509Z",
     "shell.execute_reply": "2023-04-24T06:08:33.538333Z",
     "shell.execute_reply.started": "2023-04-24T06:08:33.528732Z"
    }
   },
   "outputs": [],
   "source": [
    "per = torch.randperm(data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89eeb891-3909-4d28-9cda-07c56b53d64f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T06:44:54.432301Z",
     "iopub.status.busy": "2023-04-24T06:44:54.431432Z",
     "iopub.status.idle": "2023-04-24T07:13:41.678674Z",
     "shell.execute_reply": "2023-04-24T07:13:41.677262Z",
     "shell.execute_reply.started": "2023-04-24T06:44:54.432256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 326294302.67658997 4273 2.5232811512728603\n",
      "epoch=1 326372319.4695916 4350 2.5687509964982316\n",
      "epoch=2 326097088.06268835 4324 2.553397542266288\n",
      "epoch=3 326572505.798996 4365 2.577608758555122\n",
      "epoch=4 326675604.1150408 4244 2.506156144629539\n",
      "epoch=5 326140412.3133693 4409 2.603591527255334\n",
      "epoch=6 326823382.5893841 4263 2.5173759765682666\n",
      "epoch=7 326278767.6821778 4327 2.555169094677666\n",
      "epoch=8 326818438.9265218 4290 2.5333199482706696\n",
      "epoch=9 326441913.9854653 4252 2.5108802843932136\n",
      "epoch=10 326505077.26677394 4279 2.5268242560956167\n",
      "epoch=11 326703572.72160435 4368 2.5793803109665\n",
      "epoch=12 326655642.3296597 4354 2.571113066380069\n",
      "epoch=13 326309872.6503229 4226 2.4955268301612703\n",
      "epoch=14 326368298.9786763 4272 2.522690633802401\n",
      "epoch=15 326572511.3947983 4354 2.571113066380069\n",
      "epoch=16 326328794.6280587 4295 2.5362725356229663\n",
      "epoch=17 326508996.8394308 4361 2.5752466886732845\n",
      "epoch=18 326868802.4372189 4307 2.543358745268479\n",
      "epoch=19 326404487.56867194 4289 2.5327294308002104\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "model = GIN_dc(nfeat=data.num_node_features, n_se=n_se, nhid=64, nclass=num_classes, nlayer=3, dropout=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "batch_size = 1024\n",
    "data.to(device)\n",
    "start = 0\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "for epoch in range(20):\n",
    "    acc_sum = 0\n",
    "    total_loss = 0\n",
    "    all = torch.randperm(data.num_nodes)\n",
    "    for batch_num in range(data.num_nodes//batch_size):\n",
    "    # batch = random.choices(all, k=len(all)//20)\n",
    "        model.train()\n",
    "        model.to(data.x.device)\n",
    "        data.to(device)\n",
    "        # print(f'epoch={epoch}')\n",
    "        loss = torch.tensor([0.0], device='cuda:0')\n",
    "    # loss.to(device)\n",
    "    # print(loss.device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "    # break\n",
    "#     print(out.shape, data.y.shape)\n",
    "#     loss = model.loss(out, data.y)\n",
    "        out.to(device)\n",
    "    # for i in range(out.shape[0]): \n",
    "        batch = per[batch_size*batch_num:batch_size*(batch_num+1)]\n",
    "        for i in batch :\n",
    "        # i_tensor = torch.tensor([i], dtype=torch.long, device=out.device)\n",
    "            loss += model.loss(out[i], data.y[i])\n",
    "            total_loss += loss.item()\n",
    "        # loss += model.loss(out[i], data.y[i])\n",
    "# Convert one-hot encoded labels to class indices\n",
    "# Calculate number of correct predictions\n",
    "        acc_sum = out.max(dim=1)[1].eq(data.y.max(dim=1)[1]).sum().item()\n",
    "        # acc_sum += acc\n",
    "        # if start == 0 : print(f'epoch={epoch}', total_loss, acc_sum, acc_sum*100/data.num_nodes)\n",
    "        # start = 1\n",
    "        # print(loss.item(), acc_sum, acc_sum*100/data.num_nodes)\n",
    "    # break\n",
    "        loss.backward()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "    print(f'epoch={epoch}', total_loss, acc_sum, acc_sum*100/data.num_nodes)\n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d00ffee-165b-4290-84ac-bea9d7ad1b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T07:52:32.387404Z",
     "iopub.status.busy": "2023-04-24T07:52:32.386198Z",
     "iopub.status.idle": "2023-04-24T08:14:49.443688Z",
     "shell.execute_reply": "2023-04-24T08:14:49.442733Z",
     "shell.execute_reply.started": "2023-04-24T07:52:32.387374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 641413.125 1972 1.1645004517458648\n",
      "epoch-1 20512636928.0 19722 11.646185552399567\n",
      "epoch-2 3391872256.0 27211 16.06857088866974\n",
      "epoch-3 87146088.0 11449 6.760834519289253\n",
      "epoch-4 18804420.0 16257 9.600042517257872\n",
      "epoch-5 8231242.5 15337 9.056766444435258\n",
      "epoch-6 23866962.0 22666 13.384668985431935\n",
      "epoch-7 11963941.0 18058 10.663564481555186\n",
      "epoch-8 2806580.0 6827 4.031462770826075\n",
      "epoch-9 12034076.0 12444 7.3483994023963195\n",
      "epoch-10 7597797.5 11175 6.599032732383388\n",
      "epoch-11 5239005.5 15524 9.16719321141116\n",
      "epoch-12 2112908.25 14393 8.49931795232162\n",
      "epoch-13 2157015.0 7246 4.278889590948548\n",
      "epoch-14 1955356.625 7441 4.394040497688124\n",
      "epoch-15 701278.5 7146 4.219837843902612\n",
      "epoch-16 660327.625 7869 4.646781975044732\n",
      "epoch-17 655664.6875 7869 4.646781975044732\n",
      "epoch-18 653521.3125 7869 4.646781975044732\n",
      "epoch-19 650977.3125 7869 4.646781975044732\n"
     ]
    }
   ],
   "source": [
    "model = GIN_dc(nfeat=data.num_node_features, n_se=n_se, nhid=64, nclass=num_classes, nlayer=3, dropout=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    loss = torch.tensor([0.0], device='cuda:0')\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "\n",
    "    for i in range(out.shape[0]) : loss += model.loss(out[i], data.y[i])\n",
    "    acc_sum = out.max(dim=1)[1].eq(data.y.max(dim=1)[1]).sum().item()\n",
    "    print(f'epoch-{epoch}',loss.item(), acc_sum, acc_sum*100/data.num_nodes)    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bec68cb-e7b9-42d8-94fe-474d713df774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T08:19:06.046369Z",
     "iopub.status.busy": "2023-04-24T08:19:06.045483Z",
     "iopub.status.idle": "2023-04-24T08:40:59.003785Z",
     "shell.execute_reply": "2023-04-24T08:40:59.003015Z",
     "shell.execute_reply.started": "2023-04-24T08:19:06.046330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 630005.375 5898 3.482872040769326\n",
      "epoch-1 634949.9375 18939 11.183810373029885\n",
      "epoch-2 583449.6875 12938 7.640115032803245\n",
      "epoch-3 541708.5625 20046 11.837513212828402\n",
      "epoch-4 531122.25 24439 14.431656460556386\n",
      "epoch-5 512873.0 27899 16.474846908345782\n",
      "epoch-6 511581.90625 29366 17.34113603750967\n",
      "epoch-7 507860.375 28442 16.79549789480522\n",
      "epoch-8 500878.90625 27748 16.38567877030642\n",
      "epoch-9 498798.375 29346 17.329325688100482\n",
      "epoch-10 493691.71875 31882 18.826877993185427\n",
      "epoch-11 492156.75 31336 18.504455454314616\n",
      "epoch-12 490911.5625 30795 18.1849855027961\n",
      "epoch-13 489868.4375 31107 18.36922695357942\n",
      "epoch-14 487555.0625 31981 18.885339222760905\n",
      "epoch-15 486268.84375 33047 19.514830846270588\n",
      "epoch-16 485384.6875 33480 19.770524910979493\n",
      "epoch-17 484773.125 32817 19.379011828064932\n",
      "epoch-18 483932.1875 32917 19.43806357511087\n",
      "epoch-19 483129.1875 33163 19.583330872843874\n"
     ]
    }
   ],
   "source": [
    "model = GIN_dc(nfeat=data.num_node_features, n_se=n_se, nhid=64, nclass=num_classes, nlayer=2, dropout=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    loss = torch.tensor([0.0], device='cuda:0')\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "\n",
    "    for i in range(out.shape[0]) : loss += model.loss(out[i], data.y[i])\n",
    "    acc_sum = out.max(dim=1)[1].eq(data.y.max(dim=1)[1]).sum().item()\n",
    "    print(f'epoch-{epoch}',loss.item(), acc_sum, acc_sum*100/data.num_nodes)    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f576ad2-c723-4477-b0bb-217f597416cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d7406-a82d-40fd-b8a5-1832d452f742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69576960-2f5b-40be-8bc8-62f91894417f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149c9a2-013d-4943-9957-179a8524ce55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b2f6f-d131-4935-94ff-a878ab49f398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c32c50-c71d-4c81-b4e6-8e1e13d7954c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63587477-a332-459a-be96-dcea8781fd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9a422-f6d3-437e-8524-2a52f04fccd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f9816-c184-4373-bb05-51066409a94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b097e41-84da-438f-be84-0fc253ec0101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34881082-5da7-473b-b7bf-45c494439ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ee6a3-10f9-492e-af49-5e2c456711cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29f11c-d127-4f59-9d68-557974a56182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b974f9-d818-4e50-811b-d49573192d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84e2ef-dd04-4bab-ab8a-b4872b80d415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32ab78-d622-45dc-b542-208f63b312d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ee18e-d550-4c9f-ab2f-513ffee961d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ab108-95d8-4855-ad2a-fa9cc19159d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05aa4a7-ec31-43e3-914c-c94fdf2308c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c5203-06d7-4cce-b7e2-30580d36c2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bb0f3-658a-47a1-9766-db18c45c5672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d94915-da66-4941-b7e6-ac4e2b48bd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d0fc3-147d-4184-8874-71e1ec37a031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965cafd-1746-4849-96cf-9797a8250b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b3424-4a6d-446b-aee1-d63ec5e14cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac120f1-cb45-4167-b43a-b8f65d9063f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e792286-7a0d-44aa-a61e-62080f063560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1e093-d7c5-4b65-8cc1-59c26f50cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using device:', device)\n",
    "# print()\n",
    "\n",
    "# #Additional Info when using cuda\n",
    "# if device.type == 'cuda':\n",
    "#     print(torch.cuda.get_device_name(0))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b06bdd-899e-4951-afae-1ae1142778f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from random import choices\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import torch\n",
    "# from torch_geometric.datasets import TUDataset\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from torch_geometric.transforms import OneHotDegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1567b1-f031-4803-80fa-d9e02b43c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboardX\n",
    "# !pip install networkx\n",
    "# !pip install tensorflow\n",
    "# !pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8640c57-c1bd-48db-a960-0945e83f83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch \n",
    "# !pip install torch_geometric \n",
    "# !pip install torch_scatter\n",
    "# !pip install pymetis \n",
    "# !pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404a2fa-933d-4289-8f8d-2c0c4e4efc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_clients = 4\n",
    "# dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "# graph = dataset[0]\n",
    "# num_nodes = graph.num_nodes\n",
    "# num_edges = graph.num_edges\n",
    "# num_edges, num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4590e-f447-497f-9eb6-c30cfa92911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx_graph = utils.to_networkx(graph)\n",
    "# partitions = pymetis.part_graph(num_clients, adjacency=nx.to_dict_of_lists(nx_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4baa3-64ef-4d92-ba66-555e83cbc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitions_np = np.array(partitions[1])\n",
    "# partition_tensor = torch.from_numpy(partitions_np)\n",
    "# subgraphs = []\n",
    "# for i in range(num_clients):\n",
    "#     nodes = (partition_tensor == i).nonzero(as_tuple=True)[0]\n",
    "#     subgraph = graph.subgraph(nodes)\n",
    "#     subgraphs.append(subgraph)\n",
    "#     print(f'Number of nodes = {subgraph.num_nodes} and Number of edges = {subgraph.num_edges}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6d59a3-99f5-49dc-b293-ffc1158fde7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T13:37:48.232779Z",
     "iopub.status.busy": "2023-04-22T13:37:48.231511Z",
     "iopub.status.idle": "2023-04-22T13:40:10.553433Z",
     "shell.execute_reply": "2023-04-22T13:40:10.552488Z",
     "shell.execute_reply.started": "2023-04-22T13:37:48.232723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-22 13:37:48--  https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip\n",
      "Resolving s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)... 52.219.124.178\n",
      "Connecting to s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)|52.219.124.178|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 895569552 (854M) [binary/octet-stream]\n",
      "Saving to: ‘datasetb2d9982.zip’\n",
      "\n",
      "datasetb2d9982.zip  100%[===================>] 854.08M  6.16MB/s    in 2m 16s  \n",
      "\n",
      "2023-04-22 13:40:10 (6.29 MB/s) - ‘datasetb2d9982.zip’ saved [895569552/895569552]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5cf1113-7e98-4426-b85b-c0ad1dce1da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T13:40:10.555348Z",
     "iopub.status.busy": "2023-04-22T13:40:10.555095Z",
     "iopub.status.idle": "2023-04-22T13:40:26.429264Z",
     "shell.execute_reply": "2023-04-22T13:40:26.428563Z",
     "shell.execute_reply.started": "2023-04-22T13:40:10.555324Z"
    }
   },
   "outputs": [],
   "source": [
    "# from zipfile import ZipFile\n",
    "  \n",
    "# # loading the temp.zip and creating a zip object\n",
    "# with ZipFile('datasetb2d9982.zip', 'r') as zObject:\n",
    "#     zObject.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae11a2fd-ea55-4ceb-ab6f-5626486e8afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T14:37:14.879797Z",
     "iopub.status.busy": "2023-04-22T14:37:14.879449Z",
     "iopub.status.idle": "2023-04-22T14:37:15.592326Z",
     "shell.execute_reply": "2023-04-22T14:37:15.591610Z",
     "shell.execute_reply.started": "2023-04-22T14:37:14.879773Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc118531-9099-418e-b591-661b8b27e8f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T14:37:15.593861Z",
     "iopub.status.busy": "2023-04-22T14:37:15.593529Z",
     "iopub.status.idle": "2023-04-22T14:37:40.953202Z",
     "shell.execute_reply": "2023-04-22T14:37:40.952232Z",
     "shell.execute_reply.started": "2023-04-22T14:37:15.593839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>BULLET_POINTS</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRODUCT_TYPE_ID</th>\n",
       "      <th>PRODUCT_LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1925202</th>\n",
       "      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n",
       "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1650</td>\n",
       "      <td>2125.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673191</th>\n",
       "      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n",
       "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2755</td>\n",
       "      <td>393.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765088</th>\n",
       "      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n",
       "      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n",
       "      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n",
       "      <td>7537</td>\n",
       "      <td>748.031495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594019</th>\n",
       "      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n",
       "      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n",
       "      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n",
       "      <td>2996</td>\n",
       "      <td>787.401574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283658</th>\n",
       "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6112</td>\n",
       "      <td>598.424000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        TITLE  \\\n",
       "PRODUCT_ID                                                      \n",
       "1925202     ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
       "2673191     Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
       "2765088     PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
       "1594019     ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
       "283658      The United Empire Loyalists: A Chronicle of th...   \n",
       "\n",
       "                                                BULLET_POINTS  \\\n",
       "PRODUCT_ID                                                      \n",
       "1925202     [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
       "2673191     [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
       "2765088     [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
       "1594019     [Made By 95%cotton and 5% Lycra which gives yo...   \n",
       "283658                                                    NaN   \n",
       "\n",
       "                                                  DESCRIPTION  \\\n",
       "PRODUCT_ID                                                      \n",
       "1925202                                                   NaN   \n",
       "2673191                                                   NaN   \n",
       "2765088     Specifications: Color: Red, Material: Aluminiu...   \n",
       "1594019     AISHAH Women's Lycra Cotton Ankel Leggings. Br...   \n",
       "283658                                                    NaN   \n",
       "\n",
       "            PRODUCT_TYPE_ID  PRODUCT_LENGTH  \n",
       "PRODUCT_ID                                   \n",
       "1925202                1650     2125.980000  \n",
       "2673191                2755      393.700000  \n",
       "2765088                7537      748.031495  \n",
       "1594019                2996      787.401574  \n",
       "283658                 6112      598.424000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv('dataset/train.csv', index_col='PRODUCT_ID')\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bb8078-2a20-4ad4-800d-ceb5af361fe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T14:37:40.954850Z",
     "iopub.status.busy": "2023-04-22T14:37:40.954600Z",
     "iopub.status.idle": "2023-04-22T14:37:43.011445Z",
     "shell.execute_reply": "2023-04-22T14:37:43.010519Z",
     "shell.execute_reply.started": "2023-04-22T14:37:40.954830Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# train['DESCRIPTION'].fillna(train['BULLET_POINTS'], inplace=True)\n",
    "\n",
    "# # Fill description with title\n",
    "# train['DESCRIPTION'].fillna(train['TITLE'], inplace=True)\n",
    "\n",
    "# # Fill title with description\n",
    "# train['TITLE'].fillna(train['DESCRIPTION'], inplace=True)\n",
    "\n",
    "# # Replace remaining NaN values with \"Undefined\"\n",
    "# train.fillna('Undefined', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf68201-354b-4122-a481-6e309ad9b9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T14:37:43.012857Z",
     "iopub.status.busy": "2023-04-22T14:37:43.012579Z",
     "iopub.status.idle": "2023-04-22T14:37:49.869349Z",
     "shell.execute_reply": "2023-04-22T14:37:49.868187Z",
     "shell.execute_reply.started": "2023-04-22T14:37:43.012828Z"
    }
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import entropy\n",
    "# import pandas as pd\n",
    "\n",
    "# train['product_type_entropy'] = train.groupby('PRODUCT_TYPE_ID')['PRODUCT_LENGTH'].apply(lambda x: entropy(x.value_counts(normalize=True)))\n",
    "\n",
    "# # Fill NaN values in product_type_entropy column with a default value (e.g., 0)\n",
    "# train['product_type_entropy'].fillna(0, inplace=True)\n",
    "\n",
    "# # Rank the unique values of PRODUCT_TYPE_ID based on entropy\n",
    "# ranked_product_types = train.groupby('PRODUCT_TYPE_ID')['product_type_entropy'].rank(ascending=False, method='dense')\n",
    "\n",
    "# # Create a DataFrame with one-hot encoding\n",
    "# one_hot_encoding = pd.pivot_table(train, index='PRODUCT_TYPE_ID', columns=ranked_product_types.astype(int), values='product_type_entropy', aggfunc=lambda x: 1).fillna(0).astype(int)\n",
    "\n",
    "# # # Create a dictionary to map PRODUCT_TYPE_ID to one-hot vector encoding\n",
    "# # product_type_id_to_one_hot = dict(zip(one_hot_encoding.index, one_hot_encoding.values.tolist()))\n",
    "\n",
    "# # # Apply the one-hot encoding to the PRODUCT_TYPE_ID column in the DataFrame\n",
    "# # train_temp = pd.concat([train.drop('product_type_entropy', axis=1), one_hot_encoding], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b3c4e5-1c73-4d21-9454-703827b49d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T14:37:49.871375Z",
     "iopub.status.busy": "2023-04-22T14:37:49.871127Z",
     "iopub.status.idle": "2023-04-22T14:37:49.887173Z",
     "shell.execute_reply": "2023-04-22T14:37:49.886384Z",
     "shell.execute_reply.started": "2023-04-22T14:37:49.871354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>product_type_entropy</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRODUCT_TYPE_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13416</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13417</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13419</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13420</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12907 rows × 283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "product_type_entropy  1    2    3    4    5    6    7    8    9    10   ...  \\\n",
       "PRODUCT_TYPE_ID                                                         ...   \n",
       "0                       1    1    1    1    1    1    1    1    1    1  ...   \n",
       "1                       1    1    1    1    1    1    1    1    1    1  ...   \n",
       "2                       1    0    0    0    0    0    0    0    0    0  ...   \n",
       "3                       1    0    0    0    0    0    0    0    0    0  ...   \n",
       "4                       1    0    0    0    0    0    0    0    0    0  ...   \n",
       "...                   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "13416                   1    0    0    0    0    0    0    0    0    0  ...   \n",
       "13417                   1    0    0    0    0    0    0    0    0    0  ...   \n",
       "13418                   1    0    0    0    0    0    0    0    0    0  ...   \n",
       "13419                   1    0    0    0    0    0    0    0    0    0  ...   \n",
       "13420                   1    0    0    0    0    0    0    0    0    0  ...   \n",
       "\n",
       "product_type_entropy  274  275  276  277  278  279  280  281  282  283  \n",
       "PRODUCT_TYPE_ID                                                         \n",
       "0                       0    0    0    0    0    0    0    0    0    0  \n",
       "1                       1    1    1    1    1    1    1    1    1    1  \n",
       "2                       0    0    0    0    0    0    0    0    0    0  \n",
       "3                       0    0    0    0    0    0    0    0    0    0  \n",
       "4                       0    0    0    0    0    0    0    0    0    0  \n",
       "...                   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "13416                   0    0    0    0    0    0    0    0    0    0  \n",
       "13417                   0    0    0    0    0    0    0    0    0    0  \n",
       "13418                   0    0    0    0    0    0    0    0    0    0  \n",
       "13419                   0    0    0    0    0    0    0    0    0    0  \n",
       "13420                   0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[12907 rows x 283 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a18aed-3dc4-44c4-b082-319a061e50af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T14:37:49.888671Z",
     "iopub.status.busy": "2023-04-22T14:37:49.888403Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create a dictionary to map PRODUCT_TYPE_ID to one-hot vector encoding\n",
    "# product_type_id_to_one_hot = dict(zip(one_hot_encoding.index, one_hot_encoding.values.tolist()))\n",
    "\n",
    "# # Apply the one-hot encoding to the PRODUCT_TYPE_ID column in the DataFrame\n",
    "# def map_product_type_id_to_one_hot(product_type_id):\n",
    "#     return product_type_id_to_one_hot.get(product_type_id, [0, 0, 0])\n",
    "\n",
    "# # Apply the function to the PRODUCT_TYPE_ID column to get the one-hot vector\n",
    "# train['one_hot_encoding'] = train['PRODUCT_TYPE_ID'].apply(map_product_type_id_to_one_hot)\n",
    "\n",
    "# # Split the one-hot encoding into separate columns for each element of the vector\n",
    "# train_temp = pd.concat([train.drop('one_hot_encoding', axis=1), train['one_hot_encoding'].apply(pd.Series)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e9640-6595-40c7-9278-d43d62a1fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39d576b-1ece-4a06-81f6-42133f4e056e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T13:56:35.424438Z",
     "iopub.status.busy": "2023-04-22T13:56:35.423811Z",
     "iopub.status.idle": "2023-04-22T13:56:35.442866Z",
     "shell.execute_reply": "2023-04-22T13:56:35.441995Z",
     "shell.execute_reply.started": "2023-04-22T13:56:35.424409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>BULLET_POINTS</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRODUCT_TYPE_ID</th>\n",
       "      <th>PRODUCT_LENGTH</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1925202</th>\n",
       "      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n",
       "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
       "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>2125.980000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673191</th>\n",
       "      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n",
       "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
       "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>393.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765088</th>\n",
       "      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n",
       "      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n",
       "      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n",
       "      <td>7537.0</td>\n",
       "      <td>748.031495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594019</th>\n",
       "      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n",
       "      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n",
       "      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n",
       "      <td>2996.0</td>\n",
       "      <td>787.401574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283658</th>\n",
       "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
       "      <td>6112.0</td>\n",
       "      <td>598.424000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     TITLE  \\\n",
       "1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
       "2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
       "2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
       "1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
       "283658   The United Empire Loyalists: A Chronicle of th...   \n",
       "\n",
       "                                             BULLET_POINTS  \\\n",
       "1925202  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
       "2673191  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
       "2765088  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
       "1594019  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
       "283658                                           Undefined   \n",
       "\n",
       "                                               DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
       "1925202  [LUXURIOUS & APPEALING: Beautiful custom-made ...           1650.0   \n",
       "2673191  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...           2755.0   \n",
       "2765088  Specifications: Color: Red, Material: Aluminiu...           7537.0   \n",
       "1594019  AISHAH Women's Lycra Cotton Ankel Leggings. Br...           2996.0   \n",
       "283658   The United Empire Loyalists: A Chronicle of th...           6112.0   \n",
       "\n",
       "         PRODUCT_LENGTH   1   2   3   4   5  ...  274  275  276  277  278  \\\n",
       "1925202     2125.980000 NaN NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "2673191      393.700000 NaN NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "2765088      748.031495 NaN NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "1594019      787.401574 NaN NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "283658       598.424000 NaN NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "         279  280  281  282  283  \n",
       "1925202  NaN  NaN  NaN  NaN  NaN  \n",
       "2673191  NaN  NaN  NaN  NaN  NaN  \n",
       "2765088  NaN  NaN  NaN  NaN  NaN  \n",
       "1594019  NaN  NaN  NaN  NaN  NaN  \n",
       "283658   NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 288 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f994d-9319-4e31-8d82-a444884f373b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T13:40:48.965164Z",
     "iopub.status.busy": "2023-04-22T13:40:48.964936Z"
    }
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import entropy\n",
    "\n",
    "\n",
    "# # Calculate entropy for each unique value in the PRODUCT_TYPE_ID column\n",
    "# product_type_entropy = train.groupby('PRODUCT_TYPE_ID').apply(lambda x: entropy(x['PRODUCT_LENGTH'].value_counts(normalize=True)))\n",
    "\n",
    "# # Sort the unique values of PRODUCT_TYPE_ID based on entropy in descending order\n",
    "# sorted_product_type_ids = product_type_entropy.sort_values(ascending=False).index\n",
    "\n",
    "# # Assign one-hot vector encoding based on the sorted PRODUCT_TYPE_ID values\n",
    "# one_hot_encoding = np.eye(len(sorted_product_type_ids), dtype=int)\n",
    "\n",
    "# # Create a dictionary to map PRODUCT_TYPE_ID to one-hot vector encoding\n",
    "# product_type_id_to_one_hot = dict(zip(sorted_product_type_ids, one_hot_encoding))\n",
    "\n",
    "# # Apply the one-hot encoding to the PRODUCT_TYPE_ID column in the DataFrame\n",
    "# train['one_hot_encoding'] = train['PRODUCT_TYPE_ID'].map(product_type_id_to_one_hot)\n",
    "\n",
    "# # Split the one-hot encoding into separate columns for each element of the vector\n",
    "# train_temp = pd.concat([train.drop('one_hot_encoding', axis=1), train['one_hot_encoding'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5ffb9-593d-49a0-aaa3-e23ddc028044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ca514-351a-4953-90d3-f3ddce13fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbf796-5374-422a-8e3b-03427833e150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
