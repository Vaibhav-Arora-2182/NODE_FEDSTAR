{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca952887",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f23449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generic\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "from random import choices\n",
    "\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GINConv, global_add_pool, SAGEConv\n",
    "from torch_geometric.transforms import OneHotDegree\n",
    "from torch_geometric.utils import to_networkx, degree, to_dense_adj, to_scipy_sparse_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse as sp\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "from torch_geometric.utils import to_networkx, subgraph\n",
    "import torch_geometric.utils as utils\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "#utility\n",
    "import networkx as nx\n",
    "from dtaidistance import dtw\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pymetis\n",
    "from ogb.nodeproppred import PygNodePropPredDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9080c",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee082d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "num_clients = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "alg = 'fedstar'\n",
    "num_rounds = 20\n",
    "local_epoch = 10\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "nlayer = 3 # number of GINConv layers\n",
    "hidden = 64\n",
    "dropout = 0.5\n",
    "batch_size = 128  # not used\n",
    "seed = 69\n",
    "datapath = '.Data'\n",
    "outbase = 'outputs'\n",
    "data_group = 'arxiv'\n",
    "n_rw = 16\n",
    "n_dg = 16\n",
    "n_ones = 16\n",
    "type_init = 'rw_dg' #options are rw, dg and rw_dg\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1cbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dataSplit = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16273f8",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf19f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numGraphLabels(g):\n",
    "    s = set(g.y.flatten().tolist())\n",
    "    return len(s)\n",
    "\n",
    "def init_structure_encoding(  g, type_init = 'rw_dg'):\n",
    "\n",
    "    if type_init == 'rw':\n",
    "        # Geometric diffusion features with Random Walk\n",
    "        A = to_scipy_sparse_matrix(g.edge_index, num_nodes=g.num_nodes)\n",
    "        D = (degree(g.edge_index[0], num_nodes=g.num_nodes) ** -1.0).numpy()\n",
    "\n",
    "        Dinv=sp.diags(D)\n",
    "        RW=A*Dinv\n",
    "        M=RW\n",
    "\n",
    "        SE_rw=[torch.from_numpy(M.diagonal()).float()]\n",
    "        M_power=M\n",
    "        for _ in range(n_rw-1):\n",
    "            M_power=M_power*M\n",
    "            SE_rw.append(torch.from_numpy(M_power.diagonal()).float())\n",
    "        SE_rw=torch.stack(SE_rw,dim=-1)\n",
    "\n",
    "        g['stc_enc'] = SE_rw\n",
    "\n",
    "    elif type_init == 'dg':\n",
    "        # PE_degree\n",
    "        g_dg = (degree(g.edge_index[0], num_nodes=g.num_nodes)).numpy().clip(1, n_dg)\n",
    "        SE_dg = torch.zeros([g.num_nodes, n_dg])\n",
    "        for i in range(len(g_dg)):\n",
    "            SE_dg[i,int(g_dg[i]-1)] = 1\n",
    "\n",
    "        g['stc_enc'] = SE_dg\n",
    "\n",
    "    elif type_init == 'rw_dg':\n",
    "        # SE_rw\n",
    "        A = to_scipy_sparse_matrix(g.edge_index, num_nodes=g.num_nodes)\n",
    "        D = (degree(g.edge_index[0], num_nodes=g.num_nodes) ** -1.0).numpy()\n",
    "\n",
    "        Dinv=sp.diags(D)\n",
    "        RW=A*Dinv\n",
    "        M=RW\n",
    "\n",
    "        SE=[torch.from_numpy(M.diagonal()).float()]\n",
    "        M_power=M\n",
    "        for _ in range(n_rw-1):\n",
    "            M_power=M_power*M\n",
    "            SE.append(torch.from_numpy(M_power.diagonal()).float())\n",
    "        SE_rw=torch.stack(SE,dim=-1)\n",
    "\n",
    "        # PE_degree\n",
    "        g_dg = (degree(g.edge_index[0], num_nodes=g.num_nodes)).numpy().clip(1, n_dg)\n",
    "        SE_dg = torch.zeros([g.num_nodes, n_dg])\n",
    "        for i in range(len(g_dg)):\n",
    "            SE_dg[i,int(g_dg[i]-1)] = 1\n",
    "\n",
    "        g['stc_enc'] = torch.cat([SE_rw, SE_dg], dim=1)\n",
    "\n",
    "    return g\n",
    "\n",
    "def get_stats(df, ds, graph_train, graph_val=None, graph_test=None):\n",
    "    from collections import Counter\n",
    "    labels_train = graph_train.y.flatten().tolist()\n",
    "    df.loc[ds, '#Nodes_train'] = graph_train.num_nodes\n",
    "    df.loc[ds, '#Edges_train'] = graph_train.num_edges\n",
    "    df.loc[ds, 'Avg_degree_train'] = graph_train.num_edges/graph_train.num_nodes\n",
    "    df.loc[ds, '#Labels_train'] = len(set(labels_train))\n",
    "    df.loc[ds, 'Class_dist_train'] = str(dict(Counter(labels_train)))\n",
    "    \n",
    "    if graph_test:\n",
    "        labels_test = graph_test.y.flatten().tolist()\n",
    "        df.loc[ds, '#Nodes_test'] = graph_test.num_nodes\n",
    "        df.loc[ds, '#Edges_test'] = graph_test.num_edges\n",
    "        df.loc[ds, 'Avg_degree_test'] = graph_test.num_edges/graph_test.num_nodes\n",
    "        df.loc[ds, '#Labels_test'] = len(set(labels_test))\n",
    "        df.loc[ds, 'Class_dist_test'] = str(dict(Counter(labels_test)))\n",
    "        \n",
    "    if graph_val:\n",
    "        labels_val = graph_val.y.flatten().tolist()\n",
    "        df.loc[ds, '#Nodes_val'] = graph_val.num_nodes\n",
    "        df.loc[ds, '#Edges_val'] = graph_val.num_edges\n",
    "        df.loc[ds, 'Avg_degree_val'] = graph_val.num_edges/graph_val.num_nodes\n",
    "        df.loc[ds, '#Labels_val'] = len(set(labels_val))\n",
    "        df.loc[ds, 'Class_dist_val'] = str(dict(Counter(labels_val)))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f97b85",
   "metadata": {},
   "source": [
    "# Making data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b530f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData_multiDS(num_clients, datapath,  batchSize=32, seed=None):\n",
    "\n",
    "    num_clients = num_clients\n",
    "    splitedData = {}\n",
    "    df = pd.DataFrame()\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "    graph = dataset[0]\n",
    "    num_nodes = graph.num_nodes\n",
    "    num_edges = graph.num_edges\n",
    "    nx_graph = utils.to_networkx(graph)\n",
    "    partitions = pymetis.part_graph(num_clients, adjacency=nx.to_dict_of_lists(nx_graph))\n",
    "    partitions_np = np.array(partitions[1])\n",
    "    partition_tensor = torch.from_numpy(partitions_np)\n",
    "    subgraphs = []\n",
    "    print(f'Number of nodes in the orignal graph = {num_nodes}\\nNumber of edges in the orignal graph = {num_edges}\\nAverage degree of the orignal graph = {num_edges/num_nodes}\\n')\n",
    "    train_ratio = 0.2\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.6\n",
    "    for i in range(num_clients):\n",
    "        data = f'Client{i+1}'\n",
    "        nodes = (partition_tensor == i).nonzero(as_tuple=True)[0]\n",
    "        subgraph = graph.subgraph(nodes)\n",
    "        subgraphs.append(subgraph)\n",
    "        print(f'Number of nodes = {subgraph.num_nodes} and Number of edges = {subgraph.num_edges}')\n",
    "    \n",
    "\n",
    "# #     for subgraph in subgraphs:\n",
    "        num_nodes = subgraph.num_nodes\n",
    "        train_idx, test_idx = train_test_split(range(num_nodes), test_size=test_ratio)\n",
    "        train_idx, val_idx = train_test_split(train_idx, test_size=val_ratio/(1-test_ratio))\n",
    "\n",
    "        train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "        val_idx = torch.tensor(val_idx, dtype=torch.long)\n",
    "        test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
    "\n",
    "        # Create new subgraphs with the same structure, but with the nodes split and init their struct encodings\n",
    "        train_subgraph = init_structure_encoding(subgraph.subgraph(train_idx))\n",
    "        val_subgraph = init_structure_encoding(subgraph.subgraph(val_idx))\n",
    "        test_subgraph = init_structure_encoding(subgraph.subgraph(test_idx))\n",
    "\n",
    "#         split_subgraph = []\n",
    "#         partitions_split = pymetis.part_graph(3, adjacency=nx.to_dict_of_lists(nx_graph), ubvec = (train_ratio, val_ratio, test_ratio))\n",
    "#         partitions_np_split = np.array(partitions[1])\n",
    "#         partition_tensor_split = torch.from_numpy(partitions_np_split)\n",
    "#         nodes_split = (partition_tensor_split == i).nonzero(as_tuple=True)[0]\n",
    "#         split = graph.subgraph(nodes_split)\n",
    "#         split_subgraph.append(split)\n",
    "\n",
    "        \n",
    "#         train_subgraph = split_subgraph[0]\n",
    "#         val_subgraph = split_subgraph[1]\n",
    "#         test_subgraph = split_subgraph[2]\n",
    "        \n",
    "        num_node_features = train_subgraph.num_node_features\n",
    "        num_graph_labels = get_numGraphLabels(train_subgraph)#rewritten\n",
    "        \n",
    "        \n",
    "        \n",
    "        splitedData[data] = ({'train': train_subgraph, 'val': val_subgraph, 'test': test_subgraph},\n",
    "                             num_node_features, num_graph_labels, train_subgraph.num_nodes)\n",
    "\n",
    "        df = get_stats(df, data, train_subgraph, graph_val=val_subgraph, graph_test=test_subgraph)# rewritten\n",
    "        \n",
    "        train_subgraph.y = one_hot(train_subgraph.y).squeeze(dim=1)\n",
    "        val_subgraph.y = one_hot(val_subgraph.y).squeeze(dim=1)\n",
    "        test_subgraph.y = one_hot(test_subgraph.y).squeeze(dim=1)\n",
    "\n",
    "    return splitedData, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d0a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data ...\n",
      "Number of nodes in the orignal graph = 169343\n",
      "Number of edges in the orignal graph = 1166243\n",
      "Average degree of the orignal graph = 6.886868663009395\n",
      "\n",
      "Number of nodes = 56447 and Number of edges = 170543\n",
      "Number of nodes = 56448 and Number of edges = 185667\n",
      "Number of nodes = 56448 and Number of edges = 304555\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data ...\")\n",
    "splitedData, df_stats = prepareData_multiDS( num_clients, datapath, batch_size)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8888e6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Nodes_train</th>\n",
       "      <th>#Edges_train</th>\n",
       "      <th>Avg_degree_train</th>\n",
       "      <th>#Labels_train</th>\n",
       "      <th>Class_dist_train</th>\n",
       "      <th>#Nodes_test</th>\n",
       "      <th>#Edges_test</th>\n",
       "      <th>Avg_degree_test</th>\n",
       "      <th>#Labels_test</th>\n",
       "      <th>Class_dist_test</th>\n",
       "      <th>#Nodes_val</th>\n",
       "      <th>#Edges_val</th>\n",
       "      <th>Avg_degree_val</th>\n",
       "      <th>#Labels_val</th>\n",
       "      <th>Class_dist_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Client1</th>\n",
       "      <td>11289.0</td>\n",
       "      <td>7068.0</td>\n",
       "      <td>0.626096</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{8: 670, 28: 2143, 27: 203, 10: 717, 31: 203, ...</td>\n",
       "      <td>33869.0</td>\n",
       "      <td>60862.0</td>\n",
       "      <td>1.796982</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{18: 177, 36: 960, 28: 6429, 33: 445, 34: 2602...</td>\n",
       "      <td>11289.0</td>\n",
       "      <td>6872.0</td>\n",
       "      <td>0.608734</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{27: 208, 39: 217, 16: 249, 28: 2123, 34: 801,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Client2</th>\n",
       "      <td>11289.0</td>\n",
       "      <td>7202.0</td>\n",
       "      <td>0.637966</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{24: 2652, 6: 131, 16: 1457, 8: 281, 31: 293, ...</td>\n",
       "      <td>33869.0</td>\n",
       "      <td>67300.0</td>\n",
       "      <td>1.987068</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{30: 5658, 34: 703, 31: 885, 4: 1097, 24: 7866...</td>\n",
       "      <td>11290.0</td>\n",
       "      <td>7551.0</td>\n",
       "      <td>0.668822</td>\n",
       "      <td>39.0</td>\n",
       "      <td>{30: 1821, 4: 391, 24: 2674, 16: 1531, 8: 286,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Client3</th>\n",
       "      <td>11289.0</td>\n",
       "      <td>11377.0</td>\n",
       "      <td>1.007795</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{16: 3665, 11: 43, 28: 1647, 14: 68, 2: 229, 3...</td>\n",
       "      <td>33869.0</td>\n",
       "      <td>109247.0</td>\n",
       "      <td>3.225575</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{16: 11151, 3: 250, 28: 4951, 24: 3643, 10: 86...</td>\n",
       "      <td>11290.0</td>\n",
       "      <td>13168.0</td>\n",
       "      <td>1.166342</td>\n",
       "      <td>40.0</td>\n",
       "      <td>{28: 1571, 36: 260, 27: 381, 2: 238, 1: 73, 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         #Nodes_train  #Edges_train  Avg_degree_train  #Labels_train  \\\n",
       "Client1       11289.0        7068.0          0.626096           40.0   \n",
       "Client2       11289.0        7202.0          0.637966           40.0   \n",
       "Client3       11289.0       11377.0          1.007795           40.0   \n",
       "\n",
       "                                          Class_dist_train  #Nodes_test  \\\n",
       "Client1  {8: 670, 28: 2143, 27: 203, 10: 717, 31: 203, ...      33869.0   \n",
       "Client2  {24: 2652, 6: 131, 16: 1457, 8: 281, 31: 293, ...      33869.0   \n",
       "Client3  {16: 3665, 11: 43, 28: 1647, 14: 68, 2: 229, 3...      33869.0   \n",
       "\n",
       "         #Edges_test  Avg_degree_test  #Labels_test  \\\n",
       "Client1      60862.0         1.796982          40.0   \n",
       "Client2      67300.0         1.987068          40.0   \n",
       "Client3     109247.0         3.225575          40.0   \n",
       "\n",
       "                                           Class_dist_test  #Nodes_val  \\\n",
       "Client1  {18: 177, 36: 960, 28: 6429, 33: 445, 34: 2602...     11289.0   \n",
       "Client2  {30: 5658, 34: 703, 31: 885, 4: 1097, 24: 7866...     11290.0   \n",
       "Client3  {16: 11151, 3: 250, 28: 4951, 24: 3643, 10: 86...     11290.0   \n",
       "\n",
       "         #Edges_val  Avg_degree_val  #Labels_val  \\\n",
       "Client1      6872.0        0.608734         40.0   \n",
       "Client2      7551.0        0.668822         39.0   \n",
       "Client3     13168.0        1.166342         40.0   \n",
       "\n",
       "                                            Class_dist_val  \n",
       "Client1  {27: 208, 39: 217, 16: 249, 28: 2123, 34: 801,...  \n",
       "Client2  {30: 1821, 4: 391, 24: 2674, 16: 1531, 8: 286,...  \n",
       "Client3  {28: 1571, 36: 260, 27: 381, 2: 238, 1: 73, 34...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf4ea926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to outputs/stats_trainData.csv\n"
     ]
    }
   ],
   "source": [
    "outf = os.path.join(outbase, 'stats_trainData.csv')\n",
    "df_stats.to_csv(outf)\n",
    "print(f\"Wrote to {outf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14994f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Client1': ({'train': Data(num_nodes=11289, edge_index=[2, 7068], x=[11289, 128], node_year=[11289, 1], y=[11289, 40], stc_enc=[11289, 32]),\n",
       "   'val': Data(num_nodes=11289, edge_index=[2, 6872], x=[11289, 128], node_year=[11289, 1], y=[11289, 40], stc_enc=[11289, 32]),\n",
       "   'test': Data(num_nodes=33869, edge_index=[2, 60862], x=[33869, 128], node_year=[33869, 1], y=[33869, 40], stc_enc=[33869, 32])},\n",
       "  128,\n",
       "  40,\n",
       "  11289),\n",
       " 'Client2': ({'train': Data(num_nodes=11289, edge_index=[2, 7202], x=[11289, 128], node_year=[11289, 1], y=[11289, 40], stc_enc=[11289, 32]),\n",
       "   'val': Data(num_nodes=11290, edge_index=[2, 7551], x=[11290, 128], node_year=[11290, 1], y=[11290, 40], stc_enc=[11290, 32]),\n",
       "   'test': Data(num_nodes=33869, edge_index=[2, 67300], x=[33869, 128], node_year=[33869, 1], y=[33869, 40], stc_enc=[33869, 32])},\n",
       "  128,\n",
       "  40,\n",
       "  11289),\n",
       " 'Client3': ({'train': Data(num_nodes=11289, edge_index=[2, 11377], x=[11289, 128], node_year=[11289, 1], y=[11289, 40], stc_enc=[11289, 32]),\n",
       "   'val': Data(num_nodes=11290, edge_index=[2, 13168], x=[11290, 128], node_year=[11290, 1], y=[11290, 40], stc_enc=[11290, 32]),\n",
       "   'test': Data(num_nodes=33869, edge_index=[2, 109247], x=[33869, 128], node_year=[33869, 1], y=[33869, 40], stc_enc=[33869, 32])},\n",
       "  128,\n",
       "  40,\n",
       "  11289)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cecadb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_se = n_rw + n_dg\n",
    "n_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea033f",
   "metadata": {},
   "source": [
    "# Making client and server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5236ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client_GC():\n",
    "    def __init__(self, model, client_id, client_name, train_size, graphs, optimizer, device):\n",
    "        self.model = model.to(device)\n",
    "        self.id = client_id\n",
    "        self.name = client_name\n",
    "        self.train_size = train_size\n",
    "        self.graphs = graphs\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.W = {key: value for key, value in self.model.named_parameters()}\n",
    "        self.dW = {key: torch.zeros_like(value) for key, value in self.model.named_parameters()}\n",
    "        self.W_old = {key: value.data.clone() for key, value in self.model.named_parameters()}\n",
    "\n",
    "        self.gconvNames = None\n",
    "\n",
    "        self.train_stats = ([0], [0], [0], [0])\n",
    "        self.weightsNorm = 0.\n",
    "        self.gradsNorm = 0.\n",
    "        self.convGradsNorm = 0.\n",
    "        self.convWeightsNorm = 0.\n",
    "        self.convDWsNorm = 0.\n",
    "\n",
    "    def download_from_server(self, server):\n",
    "        self.gconvNames = server.W.keys()\n",
    "        for k in server.W:\n",
    "            if '_s' in k:\n",
    "                self.W[k].data = server.W[k].data.clone()\n",
    "                \n",
    "\n",
    "    def cache_weights(self):\n",
    "        for name in self.W.keys():\n",
    "            self.W_old[name].data = self.W[name].data.clone()\n",
    "\n",
    "    def reset(self):\n",
    "        copy(target=self.W, source=self.W_old, keys=self.gconvNames)\n",
    "\n",
    "    def local_train(self, local_epoch):\n",
    "        \"\"\" For self-train & FedAvg \"\"\"\n",
    "        train_stats = train_gc(self.model, self.graphs, self.optimizer, local_epoch, self.device)\n",
    "\n",
    "        self.train_stats = train_stats\n",
    "        self.weightsNorm = torch.norm(flatten(self.W)).item()\n",
    "\n",
    "        weights_conv = {key: self.W[key] for key in self.gconvNames}\n",
    "        self.convWeightsNorm = torch.norm(flatten(weights_conv)).item()\n",
    "\n",
    "        grads = {key: value.grad for key, value in self.W.items()}\n",
    "        self.gradsNorm = torch.norm(flatten(grads)).item()\n",
    "\n",
    "        grads_conv = {key: self.W[key].grad for key in self.gconvNames}\n",
    "        self.convGradsNorm = torch.norm(flatten(grads_conv)).item()\n",
    "\n",
    "    def compute_weight_update(self, local_epoch):\n",
    "        \"\"\" For GCFL \"\"\"\n",
    "        copy(target=self.W_old, source=self.W, keys=self.gconvNames)\n",
    "\n",
    "        train_stats = train_gc(self.model, self.graphs, self.optimizer, local_epoch, self.device)\n",
    "\n",
    "        subtract_(target=self.dW, minuend=self.W, subtrahend=self.W_old)\n",
    "\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "        self.weightsNorm = torch.norm(flatten(self.W)).item()\n",
    "\n",
    "        weights_conv = {key: self.W[key] for key in self.gconvNames}\n",
    "        self.convWeightsNorm = torch.norm(flatten(weights_conv)).item()\n",
    "\n",
    "        dWs_conv = {key: self.dW[key] for key in self.gconvNames}\n",
    "        self.convDWsNorm = torch.norm(flatten(dWs_conv)).item()\n",
    "\n",
    "        grads = {key: value.grad for key, value in self.W.items()}\n",
    "        self.gradsNorm = torch.norm(flatten(grads)).item()\n",
    "\n",
    "        grads_conv = {key: self.W[key].grad for key in self.gconvNames}\n",
    "        self.convGradsNorm = torch.norm(flatten(grads_conv)).item()\n",
    "\n",
    "    def evaluate(self):\n",
    "        return eval_gc(self.model, self.graphs['test'], self.device)\n",
    "\n",
    "    \n",
    "\n",
    "def copy(target, source, keys):\n",
    "    for name in keys:\n",
    "        target[name].data = source[name].data.clone()\n",
    "\n",
    "def subtract_(target, minuend, subtrahend):\n",
    "    for name in target:\n",
    "        target[name].data = minuend[name].data.clone() - subtrahend[name].data.clone()\n",
    "\n",
    "def flatten(w):\n",
    "    return torch.cat([v.flatten() for v in w.values()])\n",
    "\n",
    "def calc_gradsNorm(gconvNames, Ws):\n",
    "    grads_conv = {k: Ws[k].grad for k in gconvNames}\n",
    "    convGradsNorm = torch.norm(flatten(grads_conv)).item()\n",
    "    return convGradsNorm\n",
    "\n",
    "def train_gc(model, graphs, optimizer, local_epoch, device):\n",
    "    losses_train, accs_train, losses_val, accs_val, losses_test, accs_test = [], [], [], [], [], []\n",
    "    train_subgraph, val_subgraph, test_subgraph = graphs['train'], graphs['val'], graphs['test']\n",
    "    model.to(device)\n",
    "    for epoch in range(local_epoch):\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0.\n",
    "        num_nodes = train_subgraph.num_nodes\n",
    "\n",
    "        acc_sum = 0\n",
    "        loss = torch.tensor([0.0])\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_subgraph)\n",
    "        label = train_subgraph.y\n",
    "        acc_sum = out.max(dim=1)[1].eq(label.max(dim=1)[1]).sum().item()\n",
    "        \n",
    "        for i in range(out.shape[0]): loss += model.loss(out[i], label[i])\n",
    "        total_loss = loss.item()/num_nodes\n",
    "#         print(total_loss, epoch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = acc_sum / num_nodes\n",
    "\n",
    "        loss_v, acc_v = eval_gc(model, val_subgraph, device)\n",
    "        loss_tt, acc_tt = eval_gc(model, test_subgraph, device)\n",
    "\n",
    "        losses_train.append(total_loss)\n",
    "        accs_train.append(acc)\n",
    "        losses_val.append(loss_v)\n",
    "        accs_val.append(acc_v)\n",
    "        losses_test.append(loss_tt)\n",
    "        accs_test.append(acc_tt)\n",
    "\n",
    "    return {'trainingLosses': losses_train, 'trainingAccs': accs_train, 'valLosses': losses_val, 'valAccs': accs_val,\n",
    "            'testLosses': losses_test, 'testAccs': accs_test}\n",
    "\n",
    "\n",
    "\n",
    "def eval_gc(model, test_graph, device):\n",
    "    model.eval()\n",
    "    loss = torch.tensor([0.0])\n",
    "    total_loss = 0.\n",
    "    acc_sum = 0.\n",
    "    num_nodes = test_graph.num_nodes\n",
    "    out = model(test_graph)\n",
    "    label = test_graph.y\n",
    "    \n",
    "    for i in range(out.shape[0]): loss += model.loss(out[i], label[i])\n",
    "    \n",
    "    total_loss = loss.item()\n",
    "#     print(out.max(dim=1)[1].eq(label.max(dim=1)[1]))\n",
    "    acc_sum = out.max(dim=1)[1].eq(label.max(dim=1)[1]).sum().item()\n",
    "#     print(out.max(dim=1)[1], label.max(dim=1)[1])\n",
    "    return total_loss/num_nodes, acc_sum/num_nodes\n",
    "\n",
    "\n",
    "class Server():\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.W = {key: value for key, value in self.model.named_parameters()}\n",
    "        self.model_cache = []\n",
    "\n",
    "    def randomSample_clients(self, all_clients, frac):\n",
    "        return random.sample(all_clients, int(len(all_clients) * frac))\n",
    "\n",
    "    def aggregate_weights(self, selected_clients):\n",
    "        # pass train_size, and weighted aggregate\n",
    "        total_size = 0\n",
    "        for client in selected_clients:\n",
    "            total_size += client.train_size\n",
    "        for k in self.W.keys():\n",
    "            self.W[k].data = torch.div(torch.sum(torch.stack([torch.mul(client.W[k].data, client.train_size) for client in selected_clients]), dim=0), total_size).clone()\n",
    "\n",
    "    def aggregate_weights_per(self, selected_clients):\n",
    "        # pass train_size, and weighted aggregate\n",
    "        total_size = 0\n",
    "        for client in selected_clients:\n",
    "            total_size += client.train_size\n",
    "        for k in self.W.keys():\n",
    "            if 'graph_convs' in k:\n",
    "                self.W[k].data = torch.div(torch.sum(torch.stack([torch.mul(client.W[k].data, client.train_size) for client in selected_clients]), dim=0), total_size).clone()\n",
    "\n",
    "    def aggregate_weights_se(self, selected_clients):\n",
    "        # pass train_size, and weighted aggregate\n",
    "        total_size = 0\n",
    "        for client in selected_clients:\n",
    "            total_size += client.train_size\n",
    "        for k in self.W.keys():\n",
    "            if '_s' in k:\n",
    "                self.W[k].data = torch.div(torch.sum(torch.stack([torch.mul(client.W[k].data, client.train_size) for client in selected_clients]), dim=0), total_size).clone()\n",
    "\n",
    "    def aggregate_weights_fe(self, selected_clients):\n",
    "        # pass train_size, and weighted aggregate\n",
    "        total_size = 0\n",
    "        for client in selected_clients:\n",
    "            total_size += client.train_size\n",
    "        for k in self.W.keys():\n",
    "            if '_s' not in k:\n",
    "                self.W[k].data = torch.div(torch.sum(torch.stack([torch.mul(client.W[k].data, client.train_size) for client in selected_clients]), dim=0), total_size).clone()\n",
    "\n",
    "\n",
    "    def compute_pairwise_similarities(self, clients):\n",
    "        client_dWs = []\n",
    "        for client in clients:\n",
    "            dW = {}\n",
    "            for k in self.W.keys():\n",
    "                dW[k] = client.dW[k]\n",
    "            client_dWs.append(dW)\n",
    "        return pairwise_angles(client_dWs)\n",
    "\n",
    "    def compute_pairwise_distances(self, seqs, standardize=False):\n",
    "        \"\"\" computes DTW distances \"\"\"\n",
    "        if standardize:\n",
    "            # standardize to only focus on the trends\n",
    "            seqs = np.array(seqs)\n",
    "            seqs = seqs / seqs.std(axis=1).reshape(-1, 1)\n",
    "            distances = dtw.distance_matrix(seqs)\n",
    "        else:\n",
    "            distances = dtw.distance_matrix(seqs)\n",
    "        return distances\n",
    "\n",
    "    def min_cut(self, similarity, idc):\n",
    "        g = nx.Graph()\n",
    "        for i in range(len(similarity)):\n",
    "            for j in range(len(similarity)):\n",
    "                g.add_edge(i, j, weight=similarity[i][j])\n",
    "        cut, partition = nx.stoer_wagner(g)\n",
    "        c1 = np.array([idc[x] for x in partition[0]])\n",
    "        c2 = np.array([idc[x] for x in partition[1]])\n",
    "        return c1, c2\n",
    "\n",
    "    def aggregate_clusterwise(self, client_clusters):\n",
    "        for cluster in client_clusters:\n",
    "            targs = []\n",
    "            sours = []\n",
    "            total_size = 0\n",
    "            for client in cluster:\n",
    "                W = {}\n",
    "                dW = {}\n",
    "                for k in self.W.keys():\n",
    "                    W[k] = client.W[k]\n",
    "                    dW[k] = client.dW[k]\n",
    "                targs.append(W)\n",
    "                sours.append((dW, client.train_size))\n",
    "                total_size += client.train_size\n",
    "            # pass train_size, and weighted aggregate\n",
    "            reduce_add_average(targets=targs, sources=sours, total_size=total_size)\n",
    "\n",
    "    def compute_max_update_norm(self, cluster):\n",
    "        max_dW = -np.inf\n",
    "        for client in cluster:\n",
    "            dW = {}\n",
    "            for k in self.W.keys():\n",
    "                dW[k] = client.dW[k]\n",
    "            update_norm = torch.norm(flatten(dW)).item()\n",
    "            if update_norm > max_dW:\n",
    "                max_dW = update_norm\n",
    "        return max_dW\n",
    "        # return np.max([torch.norm(flatten(client.dW)).item() for client in cluster])\n",
    "\n",
    "    def compute_mean_update_norm(self, cluster):\n",
    "        cluster_dWs = []\n",
    "        for client in cluster:\n",
    "            dW = {}\n",
    "            for k in self.W.keys():\n",
    "                dW[k] = client.dW[k]\n",
    "            cluster_dWs.append(flatten(dW))\n",
    "\n",
    "        return torch.norm(torch.mean(torch.stack(cluster_dWs), dim=0)).item()\n",
    "\n",
    "    def cache_model(self, idcs, params, accuracies):\n",
    "        self.model_cache += [(idcs,\n",
    "                              {name: params[name].data.clone() for name in params},\n",
    "                              [accuracies[i] for i in idcs])]\n",
    "\n",
    "def flatten(source):\n",
    "    return torch.cat([value.flatten() for value in source.values()])\n",
    "\n",
    "def pairwise_angles(sources):\n",
    "    angles = torch.zeros([len(sources), len(sources)])\n",
    "    for i, source1 in enumerate(sources):\n",
    "        for j, source2 in enumerate(sources):\n",
    "            s1 = flatten(source1)\n",
    "            s2 = flatten(source2)\n",
    "            angles[i, j] = torch.true_divide(torch.sum(s1 * s2), max(torch.norm(s1) * torch.norm(s2), 1e-12)) + 1\n",
    "\n",
    "    return angles.numpy()\n",
    "\n",
    "def reduce_add_average(targets, sources, total_size):\n",
    "    for target in targets:\n",
    "        for name in target:\n",
    "            tmp = torch.div(torch.sum(torch.stack([torch.mul(source[0][name].data, source[1]) for source in sources]), dim=0), total_size).clone()\n",
    "            target[name].data += tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f7b22",
   "metadata": {},
   "source": [
    "# Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b432de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN_dc(torch.nn.Module):\n",
    "    def __init__(self, nfeat, n_se, nhid, nclass, nlayer, dropout):\n",
    "        super(GIN_dc, self).__init__()\n",
    "        self.num_layers = nlayer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.pre = torch.nn.Sequential(torch.nn.Linear(nfeat, nhid))\n",
    "\n",
    "        self.embedding_s = torch.nn.Linear(n_se, nhid)\n",
    "\n",
    "        self.graph_convs = torch.nn.ModuleList()\n",
    "        self.nn1 = torch.nn.Sequential(torch.nn.Linear(nhid + nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "        self.graph_convs.append(GINConv(self.nn1))\n",
    "        self.graph_convs_s_gcn = torch.nn.ModuleList()\n",
    "        self.graph_convs_s_gcn.append(GCNConv(nhid, nhid))\n",
    "\n",
    "        for l in range(nlayer - 1):\n",
    "            self.nnk = torch.nn.Sequential(torch.nn.Linear(nhid + nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "            self.graph_convs.append(GINConv(self.nnk))\n",
    "            self.graph_convs_s_gcn.append(GCNConv(nhid, nhid))\n",
    "\n",
    "        self.Whp = torch.nn.Linear(nhid + nhid, nhid)\n",
    "        self.post = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU())\n",
    "        self.readout = torch.nn.Sequential(torch.nn.Linear(nhid, nclass))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, s = data.x, data.edge_index, data.stc_enc\n",
    "        x = self.pre(x)\n",
    "        s = self.embedding_s(s)\n",
    "        for i in range(len(self.graph_convs)):\n",
    "            x = torch.cat((x, s), -1)\n",
    "            x = self.graph_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            s = self.graph_convs_s_gcn[i](s, edge_index)\n",
    "            s = torch.tanh(s)\n",
    "        x = self.Whp(torch.cat((x, s), -1))\n",
    "        x = self.post(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.readout(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    def loss(self, pred, label):\n",
    "#         print(pred, label)\n",
    "        return F.nll_loss(pred, label)\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, nlayer, dropout):\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = nlayer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.pre = torch.nn.Sequential(torch.nn.Linear(nfeat, nhid))\n",
    "\n",
    "        self.graph_convs = torch.nn.ModuleList()\n",
    "        self.nn1 = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "        self.graph_convs.append(GINConv(self.nn1))\n",
    "        for l in range(nlayer - 1):\n",
    "            self.nnk = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "            self.graph_convs.append(GINConv(self.nnk))\n",
    "\n",
    "        self.post = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU())\n",
    "        self.readout = torch.nn.Sequential(torch.nn.Linear(nhid, nclass))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.pre(x)\n",
    "        for i in range(len(self.graph_convs)):\n",
    "            x = self.graph_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.post(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.readout(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "    \n",
    "    \n",
    "class serverGIN_dc(torch.nn.Module):\n",
    "    def __init__(self, n_se, nlayer, nhid):\n",
    "        super(serverGIN_dc, self).__init__()\n",
    "\n",
    "        self.embedding_s = torch.nn.Linear(n_se, nhid)\n",
    "        self.Whp = torch.nn.Linear(nhid + nhid, nhid)\n",
    "\n",
    "        self.graph_convs = torch.nn.ModuleList()\n",
    "        self.nn1 = torch.nn.Sequential(torch.nn.Linear(nhid + nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "        self.graph_convs.append(GINConv(self.nn1))\n",
    "        self.graph_convs_s_gcn = torch.nn.ModuleList()\n",
    "        self.graph_convs_s_gcn.append(GCNConv(nhid, nhid))\n",
    "\n",
    "        for l in range(nlayer - 1):\n",
    "            self.nnk = torch.nn.Sequential(torch.nn.Linear(nhid + nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "            self.graph_convs.append(GINConv(self.nnk))\n",
    "            self.graph_convs_s_gcn.append(GCNConv(nhid, nhid))\n",
    "            \n",
    "            \n",
    "class serverGIN(torch.nn.Module):\n",
    "    def __init__(self, nlayer, nhid):\n",
    "        super(serverGIN, self).__init__()\n",
    "        self.graph_convs = torch.nn.ModuleList()\n",
    "        self.nn1 = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU(),\n",
    "                                       torch.nn.Linear(nhid, nhid))\n",
    "        self.graph_convs.append(GINConv(self.nn1))\n",
    "        for l in range(nlayer - 1):\n",
    "            self.nnk = torch.nn.Sequential(torch.nn.Linear(nhid, nhid), torch.nn.ReLU(), torch.nn.Linear(nhid, nhid))\n",
    "            self.graph_convs.append(GINConv(self.nnk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b592b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_devices(splitedData) :\n",
    "    idx_clients = {}\n",
    "    clients = []\n",
    "    for idx, ds in enumerate(splitedData.keys()):\n",
    "        idx_clients[idx] = ds\n",
    "        graphs, num_node_features, num_graph_labels, train_size = splitedData[ds]\n",
    "        if alg == 'fedstar':\n",
    "            cmodel_gc = GIN_dc(num_node_features, n_se, hidden, num_graph_labels, nlayer, dropout)\n",
    "        else:\n",
    "            cmodel_gc = GIN(num_node_features, hidden, num_graph_labels, nlayer, dropout)\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, cmodel_gc.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        clients.append(Client_GC(cmodel_gc, idx, ds, train_size, graphs, optimizer, device ))\n",
    "\n",
    "    if alg == 'fedstar':\n",
    "        smodel = serverGIN_dc(n_se=n_se, nlayer=nlayer, nhid=hidden)\n",
    "    else:\n",
    "        smodel = serverGIN(nlayer=nlayer, nhid=hidden)\n",
    "    server = Server(smodel, device)\n",
    "    return clients, server, idx_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3992f72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done setting up devices.\n"
     ]
    }
   ],
   "source": [
    "init_clients, init_server, init_idx_clients = setup_devices(splitedData)\n",
    "print(\"\\nDone setting up devices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b657b372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_clients[0].device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811cfc8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3681c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selftrain_GC(clients, server, local_epoch):\n",
    "    # all clients are initialized with the same weights\n",
    "    for client in clients:\n",
    "        client.download_from_server(server)\n",
    "\n",
    "    allAccs = {}\n",
    "    for client in clients:\n",
    "        client.local_train(local_epoch)\n",
    "\n",
    "        loss, acc = client.evaluate()\n",
    "        allAccs[client.name] = [client.train_stats['trainingAccs'][-1], client.train_stats['valAccs'][-1], acc]\n",
    "        print(\"  > {} done.\".format(client.name))\n",
    "\n",
    "    return allAccs\n",
    "\n",
    "\n",
    "def run_fedavg(clients, server, COMMUNICATION_ROUNDS, local_epoch, samp=None, frac=1.0, summary_writer=None):\n",
    "    for client in clients:\n",
    "        client.download_from_server(server)\n",
    "\n",
    "    if samp is None:\n",
    "        sampling_fn = server.randomSample_clients\n",
    "        frac = 1.0\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS + 1):\n",
    "        if (c_round) % 50 == 0:\n",
    "            print(f\"  > round {c_round}\")\n",
    "\n",
    "        if c_round == 1:\n",
    "            selected_clients = clients\n",
    "        else:\n",
    "            selected_clients = sampling_fn(clients, frac)\n",
    "\n",
    "        for client in selected_clients:\n",
    "            # only get weights of graphconv layers\n",
    "            client.local_train(local_epoch)\n",
    "\n",
    "        server.aggregate_weights(selected_clients)\n",
    "        for client in selected_clients:\n",
    "            client.download_from_server(server)\n",
    "\n",
    "        # write to log files\n",
    "        if c_round % 5 == 0:\n",
    "            for idx in range(len(clients)):\n",
    "                loss, acc = clients[idx].evaluate()\n",
    "                summary_writer.add_scalar('Test/Acc/user' + str(idx + 1), acc, c_round)\n",
    "                summary_writer.add_scalar('Test/Loss/user' + str(idx + 1), loss, c_round)\n",
    "\n",
    "    frame = pd.DataFrame()\n",
    "    for client in clients:\n",
    "        loss, acc = client.evaluate()\n",
    "        frame.loc[client.name, 'test_acc'] = acc\n",
    "\n",
    "    def highlight_max(s):\n",
    "        is_max = s == s.max()\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "    fs = frame.style.apply(highlight_max).data\n",
    "    print(fs)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def run_fedstar(clients, server, COMMUNICATION_ROUNDS, local_epoch, samp=None, frac=1.0, summary_writer=None):\n",
    "    for client in clients:\n",
    "        client.download_from_server(server)\n",
    "\n",
    "    if samp is None:\n",
    "        sampling_fn = server.randomSample_clients\n",
    "        frac = 1.0\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS + 1):\n",
    "        if (c_round) % 50 == 0:\n",
    "            print(f\"  > round {c_round}\")\n",
    "\n",
    "        if c_round == 1:\n",
    "            selected_clients = clients\n",
    "        else:\n",
    "            selected_clients = sampling_fn(clients, frac)\n",
    "\n",
    "        for client in selected_clients:\n",
    "            # only get weights of graphconv layers\n",
    "            client.local_train(local_epoch)\n",
    "\n",
    "        server.aggregate_weights_se(selected_clients)\n",
    "        for client in selected_clients:\n",
    "            client.download_from_server(server)\n",
    "\n",
    "        # write to log files\n",
    "        if c_round % 5 == 0:\n",
    "            for idx in range(len(clients)):\n",
    "                loss, acc = clients[idx].evaluate()\n",
    "                summary_writer.add_scalar('Test/Acc/user' + str(idx + 1), acc, c_round)\n",
    "                summary_writer.add_scalar('Test/Loss/user' + str(idx + 1), loss, c_round)\n",
    "                print(acc, loss, c_round)\n",
    "\n",
    "    frame = pd.DataFrame()\n",
    "    for client in clients:\n",
    "        loss, acc = client.evaluate()\n",
    "        frame.loc[client.name, 'test_acc'] = acc\n",
    "\n",
    "    def highlight_max(s):\n",
    "        is_max = s == s.max()\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "    fs = frame.style.apply(highlight_max).data\n",
    "    print(fs)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6987c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_selftrain( clients, server, local_epoch):\n",
    "    print(\"Self-training ...\")\n",
    "    df = pd.DataFrame()\n",
    "    allAccs = run_selftrain_GC( clients, server, local_epoch)\n",
    "    for k, v in allAccs.items():\n",
    "        df.loc[k, [f'train_acc', f'val_acc', f'test_acc']] = v\n",
    "    print(df)\n",
    "    outfile = os.path.join(outbase, f'accuracy_'+alg+'_GC.csv')\n",
    "    df.to_csv(outfile)\n",
    "    print(f\"Wrote to file: {outfile}\")\n",
    "\n",
    "def process_fedstar( clients, server, summary_writer):\n",
    "    print(\"\\nDone setting up FedStar devices.\")\n",
    "\n",
    "    print(\"Running FedStar ...\")\n",
    "    frame = run_fedstar( clients, server, num_rounds, local_epoch, samp=None, summary_writer=summary_writer)\n",
    "    outfile = os.path.join(outbase, f'accuracy_fedstar_{type_init}_GC.csv')\n",
    "    frame.to_csv(outfile)\n",
    "    print(f\"Wrote to file: {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc353eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done setting up FedStar devices.\n",
      "Running FedStar ...\n",
      "0.0037792671764740617 0.025551088946912184 5\n",
      "0.002804924857539343 0.3713111835767959 5\n",
      "0.003306858779414804 0.02268377015347737 5\n",
      "0.0037792671764740617 0.02554325523985798 10\n",
      "0.002804924857539343 3.805158063051758 10\n",
      "0.003306858779414804 0.022406735888067814 10\n",
      "0.0037792671764740617 0.02544148552776414 15\n",
      "0.002804924857539343 4.869108273790191 15\n",
      "0.003306858779414804 0.02230782970496158 15\n",
      "0.0037792671764740617 0.025059046724604357 20\n",
      "0.002804924857539343 0.033697302704907145 20\n",
      "0.003306858779414804 0.022432062531486205 20\n",
      "         test_acc\n",
      "Client1  0.003779\n",
      "Client2  0.002805\n",
      "Client3  0.003307\n",
      "Wrote to file: outputs/accuracy_fedstar_rw_dg_GC.csv\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "sw_path = os.path.join(outbase, 'raw', 'tensorboard', f'{data_group}_{alg}_{type_init}')\n",
    "summary_writer = SummaryWriter(sw_path)\n",
    "process_fedstar( clients=copy.deepcopy(init_clients), server=copy.deepcopy(init_server), summary_writer=summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8e61ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-training ...\n",
      "  > Client1 done.\n",
      "  > Client2 done.\n",
      "  > Client3 done.\n",
      "         train_acc   val_acc  test_acc\n",
      "Client1   0.003189  0.003543  0.003779\n",
      "Client2   0.002835  0.002923  0.002805\n",
      "Client3   0.003366  0.004517  0.003307\n",
      "Wrote to file: outputs/accuracy_fedstar_GC.csv\n"
     ]
    }
   ],
   "source": [
    "process_selftrain(clients=copy.deepcopy(init_clients), server=copy.deepcopy(init_server), local_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fd98685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fedavg( clients, server, summary_writer):\n",
    "    print(\"\\nDone setting up FedAvg devices.\")\n",
    "\n",
    "    print(\"Running FedAvg ...\")\n",
    "    frame = run_fedavg( clients, server, num_rounds, local_epoch, samp=None, summary_writer=summary_writer)\n",
    "    outfile = os.path.join(outbase, f'accuracy_fedavg_{type_init}_GC.csv')\n",
    "    frame.to_csv(outfile)\n",
    "    print(f\"Wrote to file: {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7a5309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done setting up FedAvg devices.\n",
      "Running FedAvg ...\n",
      "         test_acc\n",
      "Client1  0.003779\n",
      "Client2  0.002805\n",
      "Client3  0.003307\n",
      "Wrote to file: outputs/accuracy_fedavg_rw_dg_GC.csv\n"
     ]
    }
   ],
   "source": [
    "sw_path = os.path.join(outbase, 'raw', 'tensorboard', f'{data_group}_fedavg_{type_init}')\n",
    "summary_writer_avg = SummaryWriter(sw_path)\n",
    "process_fedavg(clients=copy.deepcopy(init_clients), server=copy.deepcopy(init_server),summary_writer=summary_writer_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547c839",
   "metadata": {},
   "source": [
    "I don't understand why its giving the same accuracy in all cases even though the loss was decreasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
